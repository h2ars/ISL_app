
# ISL Recognition

## Instructions

1. Download dataset. [[Download link](https://drive.google.com/file/d/19TbXn3QXMOa0OIibBrGCDCLXFVJn_RQ8/view)]
2. Dataset should be accessiable at `./data/`
3. This project was recently presented in the Raise 2023 conference held at MAHE, Dubai. It was awarded as the best paper in the conference.
4. Conference Link - https://conference.manipal.edu/RAiSE-2023/Registration
   
5. Abstract of the paper - Sign languages are important for the deaf and hard of hearing community, as they provide a means of communication and expression. However, many people outside of the deaf community are not familiar with sign languages, which can lead to communication barriers and exclusion. These are also not universal; each country and culture have its own sign language, and some countries have multiple sign languages. For example, American Sign Language (ASL) is used in the United States and Canada, British Sign Language (BSL) is used in the United Kingdom, and French Sign Language (LSF) is used in France. There are over 300 sign languages in the world today. Indian Sign Language (ISL) is a visual language used by the deaf and hard of hearing community in India. It is a complete language, with its own grammar and syntax, and is used to convey information through hand gestures, facial expressions, and body language. ISL has its roots in the British Sign Language (BSL), which was introduced to India in the 19th century by the British colonial government. Over time, ISL has evolved into its own distinct language, with regional variations and dialects. Recognizing hand gestures in sign languages is a challenging task due to the high variability in hand shapes, movements, and orientations. Indian Sign Language uses a combination of one-handed and two- handed gestures, which makes it fundamentally different from other common sign languages like ASL. This paper aims to address the communication gap between specially abled (deaf) people who can only express themselves through the Indian sign language and those who donâ€™t understand it, thereby improving accessibility and communication for sign language users. This is achieved by using and implementing Convolutional Neural Networks on our self-made dataset. This is a necessary step as none of the existing dataset fulfills the need for real world images. We have achieved 0.0178 loss and 99% accuracy on our dataset.
